{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b><center><font size=\"4\">Keyword Extraction from Political Party Programmes - Portuguese Legislative Elections 2022</font></center></b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](http://www.ccc.ipt.pt/~ricardo)<br>\n",
    "**email:**  ricardo.campos@ipt.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [Polytechnic Institute of Tomar](http://portal2.ipt.pt/en/);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#YAKE!-Installation\" data-toc-modified-id=\"YAKE!-Installation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>YAKE! Installation</a></span></li><li><span><a href=\"#Keyword-Extraction\" data-toc-modified-id=\"Keyword-Extraction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Keyword Extraction</a></span></li><li><span><a href=\"#Text2WordCloud\" data-toc-modified-id=\"Text2WordCloud-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Text2WordCloud</a></span></li><li><span><a href=\"#Text-Analsyis\" data-toc-modified-id=\"Text-Analsyis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Text Analsyis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-common-keywords-across-the-entire-collection\" data-toc-modified-id=\"Getting-common-keywords-across-the-entire-collection-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Getting common keywords across the entire collection</a></span></li><li><span><a href=\"#Comparing-left-wing-vs-right-wing-political-parties\" data-toc-modified-id=\"Comparing-left-wing-vs-right-wing-political-parties-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Comparing left-wing vs right-wing political parties</a></span></li><li><span><a href=\"#Comparing-two-Political-Parties\" data-toc-modified-id=\"Comparing-two-Political-Parties-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Comparing two Political Parties</a></span></li><li><span><a href=\"#Determining-relevant-keywords-across-the-entire-collection\" data-toc-modified-id=\"Determining-relevant-keywords-across-the-entire-collection-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Determining relevant keywords across the entire collection</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction from Political Party Programmes using YAKE. Portuguese Legislative Elections 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout our tutorial on [medium](https://medium.com/p/dd7fdcd671c9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YAKE! Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, begin by installing yake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+http://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow begins by defining the list of political parties. For each political party, the code will read the corresponding programme and apply YAKE! to extract the top-200 keywords. The keywords are then kept on a dictionary where the key is the name of the political party and the value is a list of 200 tuples consisting of a keyword and a score of relevance, the lower the score the more relevant the keyword is. Output example: `{'ADN': [('Alternativa Democrática', 0.0010326142301984532), ('estado', 0.0021829409873160214),…..]}`. Parsed texts obtained from Github are assumed to be under a folder named `data/PoliticalPartiesProposals-Parsed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "ListOfPoliticalParties = [\"ADN\", \"BE\", \"CDS\", \"Chega\", \"ErgueTe\", \"IL\", \"Livre\", \"MAS\", \"MPT\", \"NosCid\", \"PAN\", \"PCP\", \"PS\", \"PSD\", \"RIR\", \"Volt\"]\n",
    "\n",
    "dictOfKeywords = {}\n",
    "\n",
    "for PoliticalParty in ListOfPoliticalParties:\n",
    "    path = 'data/PoliticalPartiesProposals-Parsed'\n",
    "    file = open(f'{path}/Prog_{PoliticalParty}.txt',encoding=\"utf8\")\n",
    "    text = file.read()\n",
    "\n",
    "    language = \"pt\"\n",
    "    max_ngram_size = 3\n",
    "    numOfKeywords = 200\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    \n",
    "    dictOfKeywords[PoliticalParty] = keywords\n",
    "    \n",
    "    print(f\"Done for {PoliticalParty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we can list in a pandas dataframe the top-200 keywords for the entire set of political parties considered in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth',40)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "dictOfKeywords2Pandas = {}\n",
    "\n",
    "for PoliticalParty in dictOfKeywords:\n",
    "    listOfKeywords = [kw[0] for kw in dictOfKeywords[PoliticalParty]]\n",
    "    dictOfKeywords2Pandas[PoliticalParty] = listOfKeywords\n",
    "\n",
    "pd.DataFrame(dictOfKeywords2Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text2WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to transform the text into wordclouds. To accomplish this objective, we will make use of the wordcloud python package. The code bellow exemplifies this process. In the code, we begin by defining the path where we want the wordclouds to be saved (e.g., in our case we created a folder named WordCloud under the data folder, `data/Figure`), the path where the parsed texts can be found (`data/PoliticalPartiesProposals-Parsed`) and the filename (`wc_flag.jpg`) of the background image that should support the wordcloud. Note that this image, should be under the `data/Figure` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also be aware that wordclouds are defined to distinguish between the relevance of the keywords by making use of different font size: the higher the relevance of the keyword, the larger the font size should be. However, from YAKE!'s code, we could understand that the higher the relevance the lower the score. So before we move on, we should make an adaptation of YAKE!'s score (by inverting it) such that it is ready for the wordcloud python package. The code bellow makes this adaptation, generates, shows and save the word clouds under the specified folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_figures = \"data/Figures\"\n",
    "path_texts = \"data/PoliticalPartiesProposals-Parsed\"\n",
    "background_image = \"wc_flag.jpg\"\n",
    "\n",
    "for PoliticalParty in dictOfKeywords:\n",
    "    keywords = dictOfKeywords[PoliticalParty]\n",
    "    \n",
    "    #Invert the scores of YAKE\n",
    "    keyword2WordCloud = {}\n",
    "    for keyword in keywords:\n",
    "        if keyword[1] < 0:\n",
    "            keyword2WordCloud[keyword[0]] = 1 \n",
    "        else:\n",
    "            keyword2WordCloud[keyword[0]] = 1 - keyword[1]\n",
    "\n",
    "    \n",
    "    mask = np.array(Image.open(f\"{path_figures}/{background_image}\"))\n",
    "    wordcloud = WordCloud(background_color=\"white\",contour_color='firebrick', max_font_size=100,width = 1520, height = 535, mask=mask).generate_from_frequencies(keyword2WordCloud) #Objeto que permite gerar wordcloud a partir de texto\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\") #imshow plota imagens que derivam de arrays\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"{path_figures}/{PoliticalParty}.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analsyis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting common keywords across the entire collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfKeywords = []\n",
    "for PoliticalParty in dictOfKeywords2Pandas:\n",
    "    ListOfKeywords.append(set(keyword.lower() for keyword in dictOfKeywords2Pandas[PoliticalParty]))\n",
    "\n",
    "print(ListOfKeywords[0].intersection(*ListOfKeywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing left-wing vs right-wing political parties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can compare the differences between keywords used by left-wing and right-wing political parties. To do so, we begin by doing an intersection of all the relevant keywords retrieved by left-wing parties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfLeftPoliticalParties = [\"BE\", \"Livre\", \"PAN\", \"PCP\", \"PS\"]\n",
    "\n",
    "ListOfLeftKeywords = []\n",
    "\n",
    "for PoliticalParty in ListOfLeftPoliticalParties:\n",
    "    ListOfLeftKeywords.append(set(keyword.lower() for keyword in dictOfKeywords2Pandas[PoliticalParty]))\n",
    "\n",
    "ListOfLeftKeywordsIntersected = ListOfLeftKeywords[0].intersection(*ListOfLeftKeywords)\n",
    "print(ListOfLeftKeywordsIntersected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we do the same for right-wing ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfRightPoliticalParties = [\"CDS\", \"Chega\", \"IL\", \"PSD\"]\n",
    "\n",
    "ListOfRightKeywords = []\n",
    "\n",
    "for PoliticalParty in ListOfRightPoliticalParties:\n",
    "    ListOfRightKeywords.append(set(keyword.lower() for keyword in dictOfKeywords2Pandas[PoliticalParty]))\n",
    "\n",
    "ListOfRightKeywordsIntersected = ListOfRightKeywords[0].intersection(*ListOfRightKeywords)\n",
    "print(ListOfRightKeywordsIntersected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a difference between the two sets. First, we conduct a left difference and found that keywords such as \"administração pública\" (public administration) or \"habitação\" (housing) were found as relevant in all of the programmes of the left-wing political spectrum, but not on all the right-wing parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeftDifference = ListOfLeftKeywordsIntersected - ListOfRightKeywordsIntersected\n",
    "LeftDifference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, keywords such as \"segurança\" (security) or \"qualidade\" (quality) were found in all of the programmes of the right-wing spectrum, parties, but not on all the left-wing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RightDifference = ListOfRightKeywordsIntersected - ListOfLeftKeywordsIntersected\n",
    "RightDifference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing two Political Parties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also compare two political parties (e.g., PS and PSD) to see the differences in-between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfLeftPoliticalParties = [\"PS\"]\n",
    "\n",
    "ListOfLeftKeywords = []\n",
    "\n",
    "for PoliticalParty in ListOfLeftPoliticalParties:\n",
    "    for keyword in dictOfKeywords2Pandas[PoliticalParty]:\n",
    "        ListOfLeftKeywords.append(keyword.lower())\n",
    "\n",
    "setOfLeftKeywords = set(ListOfLeftKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfRightPoliticalParties = [\"PSD\"]\n",
    "\n",
    "ListOfRightKeywords = []\n",
    "\n",
    "for PoliticalParty in ListOfRightPoliticalParties:\n",
    "    for keyword in dictOfKeywords2Pandas[PoliticalParty]:\n",
    "        ListOfRightKeywords.append(keyword.lower())\n",
    "\n",
    "setOfRightKeywords = set(ListOfRightKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setOfRightKeywords.difference(setOfLeftKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setOfLeftKeywords.intersection(setOfRightKeywords)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining relevant keywords across the entire collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing to do here is to count the number of times a word appears across the sixteen texts. Thus, instead of valuing words that occur a lot in a specific document and little in the rest of the collection (as TF.IDF does), we are interested in valuing words that occur frequently across the various texts considered. We assume YAKE! as a filtering step in this process, that enabled us to only focus on keywords that worth to have a look at, and based on this, we try to understand the most relevant keywords across the entire collection of texts. To this regard, we devise a simple formula which multiplies the term frequency (TF) of a keyword in the entire collection of documents D, by the log of the number of documents where the keyword appears (|{d ∈ D: keyword ∈ d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute this, we begin by determing the entire list of keywords (removing duplicates, after transforming each keyword in lowercase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfAllKeywords = []\n",
    "for PoliticalParty in dictOfKeywords:\n",
    "     for kw in dictOfKeywords[PoliticalParty]:\n",
    "            ListOfAllKeywords.append(kw[0].lower())\n",
    "\n",
    "SetOfAllKeywords = set(ListOfAllKeywords)\n",
    "SetOfAllKeywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we count the term frequency of each keyword together with the number of documents where the keyword appears and save this information in a dictionary (dictOfOccurrences) of the form {\"public\":[4,2],…} meaning that the keyword \"public\" appears 4 times in 2 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import collections\n",
    "\n",
    "ListOfPoliticalParties = [\"ADN\", \"BE\", \"CDS\", \"Chega\", \"ErgueTe\", \"IL\", \"Livre\", \"MAS\", \"MPT\", \"NosCid\", \"PAN\", \"PCP\", \"PS\", \"PSD\", \"RIR\", \"Volt\"]\n",
    "\n",
    "dictOfOccurrences = {}\n",
    "\n",
    "for PoliticalParty in ListOfPoliticalParties:\n",
    "    file = open(f'{path_texts}/Prog_{PoliticalParty}.txt',encoding=\"utf8\")\n",
    "    text = file.read().lower()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    uni = nltk.FreqDist(tokens)\n",
    "    bi = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    tri = nltk.FreqDist(nltk.trigrams(tokens))\n",
    "    \n",
    "    ListOfDicts = []\n",
    "    ListOfDicts.append({k:v for k,v in uni.items()})\n",
    "    ListOfDicts.append({\" \".join(k):v for k,v in bi.items()})\n",
    "    ListOfDicts.append({\" \".join(k):v for k,v in tri.items()})\n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    for d in ListOfDicts:\n",
    "        counter.update(d)\n",
    "\n",
    "    dictOfKeywords = dict(counter)\n",
    "    \n",
    "    for keyword in SetOfAllKeywords:\n",
    "        if keyword in dictOfKeywords:\n",
    "            if keyword in dictOfOccurrences:\n",
    "                dictOfOccurrences[keyword][0] += dictOfKeywords[keyword]\n",
    "                dictOfOccurrences[keyword][1] += 1\n",
    "            else:\n",
    "                dictOfOccurrences[keyword] = [dictOfKeywords[keyword], 1]\n",
    "    \n",
    "dictOfOccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having this information available, we are now ready to compute the `GlobalWeight` for each of the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "dictOfOccurrencesWeight = {}\n",
    "\n",
    "for keyword in dictOfOccurrences:\n",
    "    dictOfOccurrencesWeight[keyword] = dictOfOccurrences[keyword][0] * (math.log(dictOfOccurrences[keyword][1]))\n",
    "\n",
    "dictOfOccurrencesWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "background_image = \"wc_map.jpg\"\n",
    "\n",
    "mask = np.array(Image.open(f\"{path_figures}/{background_image}\"))\n",
    "wordcloud = WordCloud(background_color=\"white\",contour_color='firebrick', max_font_size=100,width = 1520, height = 535, mask=mask).generate_from_frequencies(dictOfOccurrencesWeight) #Objeto que permite gerar wordcloud a partir de texto\n",
    "image_colors = ImageColorGenerator(mask)\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\") #imshow plota imagens que derivam de arrays\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(f\"{path_figures}/GlobalWordCloud.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sorts the values in descending order as a means to feed a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfOccurrencesWeight_sortedByValue = {k: dictOfOccurrencesWeight[k] for k in sorted(dictOfOccurrencesWeight, key=dictOfOccurrencesWeight.get, reverse=True)}\n",
    "dictOfOccurrencesWeight_sortedByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to get a bar plot of the top-50 YAKE! keywords across all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = list(dictOfOccurrencesWeight_sortedByValue.keys())[:50]\n",
    "y = list(dictOfOccurrencesWeight_sortedByValue.values())[:50]\n",
    "\n",
    "plt.figure(figsize=(18,12))\n",
    "\n",
    "plt.bar(labels, y)\n",
    "plt.title(\"YAKE! Top-50 keywords across the entire collection\")\n",
    "plt.xlabel(\"keywords\")\n",
    "plt.ylabel(\"Global Weight\")\n",
    "plt.xticks(list(range(0,len(labels)+1)), labels, rotation =90)\n",
    "plt.yticks([]);\n",
    "plt.savefig(f\"{path_figures}/GlobalWeight.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can also observe that the keywords occupying the top positions of the dictionary mostly consist of 1-term (e.g., \"saúde\"). This is easily explained by the fact that 1-term keywords are more easily found across all the documents than 2-terms or 3-terms. As a matter of fact, all the programmes may have the keyword \"saúde\" (health), but not all of them may have the keyword \"cuidados de saúde\" (health care). It also suggest that more elaborated solutions that are not only related with the term frequency should be studied in the future (perhaps including YAKE!'s score in the process). As for now, we overcome this problem by creating two further plots for 2 and 3-term keywords by filtering the dictionary to this specific number of terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we determine the `bigramKeywords` and `trigramKeywords`, and then, based on this we will have dictionaries of bigrams and trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramKeywords = []\n",
    "trigramKeywords = []\n",
    "\n",
    "for keyword in SetOfAllKeywords:\n",
    "    if len(keyword.split()) == 2:\n",
    "        bigramKeywords.append(keyword)\n",
    "    elif len(keyword.split()) == 3:\n",
    "        trigramKeywords.append(keyword)\n",
    "        \n",
    "dictOfTrigramOccurrencesWeight = {}\n",
    "for keyword in trigramKeywords:\n",
    "    dictOfTrigramOccurrencesWeight[keyword] = dictOfOccurrencesWeight[keyword]\n",
    "    \n",
    "dictOfBigramOccurrencesWeight = {}\n",
    "for keyword in bigramKeywords:\n",
    "    dictOfBigramOccurrencesWeight[keyword] = dictOfOccurrencesWeight[keyword]\n",
    "\n",
    "dictOfTrigramOccurrencesWeight_sortedByValue = {k: dictOfTrigramOccurrencesWeight[k] for k in sorted(dictOfTrigramOccurrencesWeight, key=dictOfTrigramOccurrencesWeight.get, reverse=True)}\n",
    "dictOfBigramOccurrencesWeight_sortedByValue = {k: dictOfBigramOccurrencesWeight[k] for k in sorted(dictOfBigramOccurrencesWeight, key=dictOfBigramOccurrencesWeight.get, reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = list(dictOfBigramOccurrencesWeight_sortedByValue.keys())[:50]\n",
    "y = list(dictOfBigramOccurrencesWeight_sortedByValue.values())[:50]\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "plt.bar(labels, y)\n",
    "plt.title(\"YAKE! Top-50 keywords (2-terms) across the entire collection\")\n",
    "plt.xlabel(\"keywords\")\n",
    "plt.ylabel(\"Global Weight\")\n",
    "plt.xticks(list(range(0,len(labels)+1)), labels, rotation =90)\n",
    "plt.yticks([]);\n",
    "plt.savefig(f\"{path_figures}/GlobalWeight_2terms.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = list(dictOfTrigramOccurrencesWeight_sortedByValue.keys())[:50]\n",
    "y = list(dictOfTrigramOccurrencesWeight_sortedByValue.values())[:50]\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "plt.bar(labels, y)\n",
    "plt.title(\"YAKE! Top-50 keywords (3-terms) across the entire collection\")\n",
    "plt.xlabel(\"keywords\")\n",
    "plt.ylabel(\"Global Weight\")\n",
    "plt.xticks(list(range(0,len(labels)+1)), labels, rotation =90)\n",
    "plt.yticks([]);\n",
    "plt.savefig(f\"{path_figures}/GlobalWeight_3terms.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please cite the following works when using YAKE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-depth journal paper at Information Sciences Journal**\n",
    "\n",
    "- Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C. and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509, pp 257-289. [pdf](https://doi.org/10.1016/j.ins.2019.09.013)\n",
    "\n",
    "**ECIR'18 Best Short Paper**\n",
    "\n",
    "- Campos R., Mangaravite V., Pasquali A., Jorge A.M., Nunes C., and Jatowt A. (2018). A Text Feature Based Automatic Keyword Extraction Method for Single Documents. In: Pasi G., Piwowarski B., Azzopardi L., Hanbury A. (eds). Advances in Information Retrieval. ECIR 2018 (Grenoble, France. March 26 – 29). Lecture Notes in Computer Science, vol 10772, pp. 684 - 691. [pdf](https://link.springer.com/chapter/10.1007/978-3-319-76941-7_63)\n",
    "\n",
    "- Campos R., Mangaravite V., Pasquali A., Jorge A.M., Nunes C., and Jatowt A. (2018). YAKE! Collection-independent Automatic Keyword Extractor. In: Pasi G., Piwowarski B., Azzopardi L., Hanbury A. (eds). Advances in Information Retrieval. ECIR 2018 (Grenoble, France. March 26 – 29). Lecture Notes in Computer Science, vol 10772, pp. 806 - 810. [pdf](https://link.springer.com/chapter/10.1007/978-3-319-76941-7_80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
